# -*- coding: utf-8 -*-
"""On-Time Delivery

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ftbl1M5KyhgGQDAP_Sn9b86jfnPyc2Zp
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""## Section 1: Data Loading"""

df = pd.read_csv('/content/drive/MyDrive/Portfolio/1. Business Cases/1. Classification/Logistics/Project 1: On-Time Delivery/Data/sales_orders.csv')

df.head()

df.info()

df['order_date'] = pd.to_datetime(df['order_date'])

df['category'].unique()

df.columns

df.head()

"""## Section 2: Initial Data Cleaning"""

df.duplicated().any()

df.duplicated().sum()

df.isnull().any()

df.isnull().sum()

df[df.isnull().any(axis=1)]

"""Fill in null values of numerical columns first to recalculate the nmv for later use. This way, we can preserve more data while ensuring the dataset remains reliable."""

# price
mean_price_by_product = df.groupby('product_name')['price'].mean()

# Fill in the null values in the price column with the mean price for each product_name
def fill_price_by_product(row):
    if pd.isna(row['price']) and pd.notna(row['product_name']):
        return mean_price_by_product.get(row['product_name'], row['price'])
    return row['price']

df['price'] = df.apply(fill_price_by_product, axis=1)

# Drop rows where the price column has null values
df = df.dropna(subset=['price'])

# qty_sold
mean_quantities_by_product = df.groupby('product_name')['qty_sold'].mean()

# Fill in the null values in the qtr_sold column with the mean qtr_sold for each product_name
def fill_quantities_by_product(row):
    if pd.isna(row['qty_sold']) and pd.notna(row['product_name']):
        return mean_quantities_by_product.get(row['product_name'], row['qty_sold'])
    return row['qty_sold']

df['qty_sold'] = df.apply(fill_price_by_product, axis=1)

# Drop rows where the qty_sold column has null values
df = df.dropna(subset=['qty_sold'])

df.isnull().sum()

df['qty_sold'] = df['qty_sold'].astype(int)

df['gmv'] = df['price'] * df['qty_sold']

# shipping_fee_value
mean_shipping_fee_by_order_size = df.groupby('order_size')['shipping_fee_value'].mean()

# Fill in the null values in the shipping_fee_value column with the mean shipping_fee_value for each order_size
def fill_quantities_by_product(row):
    if pd.isna(row['shipping_fee_value']) and pd.notna(row['order_size']):
        return mean_shipping_fee_by_order_size.get(row['order_size'], row['shipping_fee_value'])
    return row['shipping_fee_value']

df['shipping_fee_value'] = df.apply(fill_price_by_product, axis=1)

# discounted_percent
mean_discounted_percent_by_product = df.groupby('product_name')['discounted_percent'].mean()

# Fill in the null values in the discounted_percent column with the mean discounted_percent for each product_name
def fill_quantities_by_product(row):
    if pd.isna(row['discounted_percent']) and pd.notna(row['product_name']):
        return mean_discounted_percent_by_product.get(row['product_name'], row['discounted_percent'])
    return row['discounted_percent']

df['discounted_percent'] = df.apply(fill_price_by_product, axis=1)

# discounted_value
df['discounted_value'] = df['gmv'] * (df['discounted_percent'] / 100)

df['discounted_value'] = round(df['discounted_value'],2)

# Recalculate NMV and ensure positive values using abs()
df['nmv'] = round(abs(df['gmv'] + df['shipping_fee_value'] - df['discounted_value']),2)

"""## Section 3: EDA

### Univariate Analysis

#### Categorical Variables
"""

df['order_status'] = df['order_status'].astype('category')
df['seller_region'] = df['seller_region'].astype('category')
df['seller_province'] = df['seller_province'].astype('category')
df['product_name'] = df['product_name'].astype('category')
df['category'] = df['category'].astype('category')
df['payment_method'] = df['payment_method'].astype('category')
df['order_size'] = df['order_size'].astype('category')

cat_col = df.select_dtypes(include='category').columns
cat_col

def univariate_cat(df, col):
    print(df[col].value_counts())
    sns.countplot(data=df, x=col, hue=col)
    plt.xticks(rotation=60)
    plt.legend([], [], frameon=False)
    plt.show()

for col in cat_col:
  print('\n* Categorical Variable:', col)
  univariate_cat(df,col)
  print()

"""**Comment:**

  * **seller_region & seller_province:** For seller_region, I will pick the top 6 regions (based on frequency or another criterion) and group the remaining 4 regions as "other regions". For seller_province, I will keep the provinces corresponding to the top 6 regions and group the provinces from the 4 regions categorized as "other regions" into a single category called "other provinces". => By grouping less frequent categories, I can avoid losing important information while still simplifying the dataset, reducing noise, and preventing overfitting. Furthermore, by grouping the dependent categories (seller_province) according to the grouped categories in seller_region, I maintain the nature of the relationship between the two features.
  * **category & product_name:** For category, I will pick the top 6 categories (based on frequency or another criterion) and group the remaining 4 categories as "other categories". For product_name, I will keep the products corresponding to the top 6 categories and group the products from the 4 categories categorized as "other categories" into a single category called "other products". => By grouping less frequent categories, I can avoid losing important information while still simplifying the dataset, reducing noise, and preventing overfitting. Furthermore, by grouping the dependent categories (product_name) according to the grouped categories in category, I maintain the nature of the relationship between the two features.

#### Numerical Variables
"""

num_col = df.select_dtypes('number').columns
num_col

def univariate_num(df,col):
  print(df[col].describe())
  print('Range: ',df[col].max()-df[col].min())
  print('Var: ',df[col].var())
  print('Skewness: ',df[col].skew())
  print('Kurtosis: ',df[col].kurt())
  fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))
  sns.boxplot(data=df,y=col,ax=ax1)
  sns.histplot(data=df,x=col,kde=True,ax=ax2)
  plt.show()

for col in num_col:
  print('\n* Numerical Variable:',col)
  univariate_num(df,col)
  print()

"""**Comment:**

  * **price:** slightly positively skewed, this feature needs to be modified a bit by cutting upper outliers to reduce its skewness
  * **gmv:** highly positively skewed along with a heavy tail, this feature needs to be modified a bit by cutting upper outliers to reduce its skewness and kurtosis
  * **discounted_value:** highly positively skewed along with a heavy tail, this feature needs to be modified a bit by cutting upper outliers to reduce its skewness and kurtosis
  * **nmv:** highly positively skewed along with a heavy tail, this feature needs to be modified a bit by cutting upper outliers to reduce its skewness and kurtosis

Note: Those above features are used to calculate nmv, therefore, it might be optimal to drop those numerical columns (except nmv) to avoid multicollinearity. However, the decision should only be made after taking the bivariate analysis into action

### Bivariate Analysis

#### Input - Target (Cat - Num)
"""

target = 'on_time_delivery'

num_col = num_col.tolist()

num_col.remove(target)

num_col

cat_col

# ANOVA

import statsmodels.api as sm
from statsmodels.formula.api import ols

def bivariate_anova(df, col1, col2):
  df_sub = df[[col1,col2]]
  plt.figure(figsize=(5,6))
  sns.boxplot(data=df_sub, x=col1, y=col2)
  plt.show()
  model = ols(f'{col2} ~ C({col1})', data=df_sub).fit()
  anova_table = sm.stats.anova_lm(model, typ=2)
  print(anova_table)

col2 = 'on_time_delivery'
for i in range(0, len(cat_col)):
  col1 = cat_col[i]
  print('\n* Numerical Variable:', col2)
  print('* Categorical Variable:', col1)
  print('\n')
  bivariate_anova(df, col1, col2)
  print()

"""**Comment:**

All p-value > 0.05 => Fail to Reject H0. This suggests that all categorical columns do not have a significant effect on the on_time_delivery variable.

#### Input - Target (Num - Num)
"""

import scipy
import scipy.stats as stats

def bivariate_correlation(df,col1,col2):
  print('Pearson Correlation: ')
  print(stats.pearsonr(df[col1], df[col2]))
  print('Spearman Correlation: ')
  print(stats.spearmanr(df[col1], df[col2]))
  sns.scatterplot(data=df,x=col1,y=col2)
  plt.show()

col1 = 'on_time_delivery'
for i in range (0, len(num_col)):
    col2 = num_col[i]
    print('\n* Numerical Variable:', col1)
    print('* Numerical Variable:', col2)
    print('\n')
    bivariate_correlation(df,col1,col2)
    print()

"""**Comment:**

All p-value > 0.05 => Fail to Reject H0. This suggests that all numerical columns are not associated with on_time_delivery column

#### Input - Input (Num - Cat)
"""

for i in range(0, len(cat_col)):
  col1 = cat_col[i]
  for j in range(i+1, len(num_col)):
    col2 = num_col[j]
    print('\n* Categorical Variable:', col1)
    print('* Numerical Variable:', col2)
    print('\n')
    bivariate_anova(df, col1, col2)
    print()

"""**Comment:**

*   **product_name & gmv:** Reject H0. This suggests that product_name column is associated with gmv column
*   **product_name & shipping_fee_value:** Reject H0. This suggests that product_name column is associated with shipping_fee_value column
*   **product_name & discounted_value:** Reject H0. This suggests that product_name column is associated with discounted_value column
*   **product_name & nmv:** Reject H0. This suggests that product_name column is associated with nmv column
*   **category & nmv:** Reject H0. This suggests that category column is associated with nmv column
*   **order_size & discounted_percent:** Reject H0. This suggests that order_size column is associated with discounted_percent column
*   **order_size & discounted_value:** Reject H0. This suggests that order_size column is associated with discounted_value column
*   **order_size & nmv:** Reject H0. This suggests that order_size column is associated with nmv column
*   **payment_method & discounted_value:** Reject H0. This suggests that payment_method column is associated with discounted_value column
*   **payment_method & nmv:** Reject H0. This suggests that payment_method column is associated with nmv column

#### Input - Input (Cat - Cat)
"""

from scipy.stats import chi2_contingency
from scipy.stats import chi2

def bivariate_contingency_table_chi_square(col1, col2):
  # Contingency table
  contingency_table = pd.crosstab(col1,col2)
  print(contingency_table)
  sns.barplot(data=contingency_table, ci=None)
  plt.show()

  # Chi-square test
  chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)
  print('dof= ', dof)
  print('p-value= ',p_value)
  alpha = 0.05
  if p_value <= alpha:
    print('Reject H0. There is a significant association between the two variables')
  else:
    print('Fail to reject H0. There is no significant association between the two variables')

for i in range (0, len(cat_col)):
  for j in range(i+1, len(cat_col)):
    col1 = cat_col[i]
    col2 = cat_col[j]
    print('\n* Categorical Variable:', col1)
    print('* Categorical Variable:', col2)
    print('\n')
    bivariate_contingency_table_chi_square(df[col1],df[col2])
    print()

"""**Comment:**

*   **seller_region & seller_province:** Reject H0. This suggests that seller_region column is associated with seller_province column
*   **product_name & category:** Reject H0. This suggests that seller_province column is associated with category column

#### Input - Input (Num - Num)
"""

for i in range (0, len(num_col)):
  for j in range(i+1, len(num_col)):
    col1 = num_col[i]
    col2 = num_col[j]
    print('\n* Numerical Variable:', col1)
    print('* Numerical Variable:', col2)
    print('\n')
    bivariate_correlation(df,col1,col2)
    print()

"""**Comment:**

*   **is_cancelled & shipping_fee_value:** Reject H0. This suggests that is_cancelled column is associated with shipping_fee_value column
*   **is_cancelled & discounted_percent:** Reject H0. This suggests that is_cancelled column is associated with discounted_percent column

Note: except is_cancelled and is_2h_delivery, all other numerical variables are highly correlated to one another. This suggest that we should keep only nmv for later use in building the model to avoid multicollinearity

## Section 4: Contextual Imputation

Based on the Chi-Square Test, we can conclude that seller_region and seller_province have a significant association with one another. Therefore, to determine and impute the null values in each column with high confidence, we can map each region to its corresponding 5 provinces and vice versa to fill in as many missing values as possible. And then, we continue to fill in missing values in the seller_province column thanks to its association with category and order_size. Next, we can use the newly added values from the seller_province to map the values in the seller_region as the first step. Finally, we will drop the remaining null values if they are existed
"""

df[['seller_region','seller_province']].groupby(['seller_region','seller_province']).count()

region_mapping = {
    'New York': {
        'provinces': ['New York County', 'Kings County', 'Queens County', 'Bronx County', 'Richmond County']
    },
    'Los Angeles': {
        'provinces': ['Los Angeles County', 'Orange County', 'Ventura County', 'Riverside County', 'San Bernardino County']
    },
    'Chicago': {
        'provinces': ['Cook County', 'DuPage County', 'Lake County', 'Will County', 'Kane County']
    },
    'Houston': {
        'provinces': ['Harris County', 'Fort Bend County', 'Montgomery County', 'Brazoria County', 'Galveston County']
    },
    'Miami': {
        'provinces': ['Miami-Dade County', 'Broward County', 'Palm Beach County', 'Monroe County', 'Collier County']
    },
    'San Francisco': {
        'provinces': ['San Francisco County', 'San Mateo County', 'Alameda County', 'Marin County', 'Contra Costa County']
    },
    'Boston': {
        'provinces': ['Suffolk County', 'Middlesex County', 'Norfolk County', 'Essex County', 'Plymouth County']
    },
    'Seattle': {
        'provinces': ['King County', 'Snohomish County', 'Pierce County', 'Kitsap County', 'Thurston County']
    },
    'Dallas': {
        'provinces': ['Dallas County', 'Collin County', 'Denton County', 'Tarrant County', 'Rockwall County']
    },
    'Atlanta': {
        'provinces': ['Fulton County', 'DeKalb County', 'Cobb County', 'Gwinnett County', 'Clayton County']
    }
}

# Create a reverse mapping of province to region
province_to_region = {}
for region, data in region_mapping.items():
    for province in data['provinces']:
        province_to_region[province] = region

# Fill null values in seller_region using seller_province
def fill_region_by_province(row):
    if pd.isna(row['seller_region']) and pd.notna(row['seller_province']):
        return province_to_region.get(row['seller_province'], row['seller_region'])
    return row['seller_region']

df['seller_region'] = df.apply(fill_region_by_province, axis=1)

# Fill null values in seller_province using seller_region
def fill_province_by_region(row):
    if pd.isna(row['seller_province']) and pd.notna(row['seller_region']):
        possible_provinces = region_mapping.get(row['seller_region'], {}).get('provinces', [])
        return np.random.choice(possible_provinces) if possible_provinces else row['seller_province']
    return row['seller_province']

df['seller_province'] = df.apply(fill_province_by_region, axis=1)

df[df['seller_province'].isna() & df['seller_region'].isna()]

df.isnull().sum()

# Ensure most_common_province_by_category is calculated beforehand
most_common_province_by_category = df.groupby('category')['seller_province'].apply(lambda x: x.mode()[0] if not x.mode().empty else np.nan)

# Apply the imputation for missing seller_province values based on category
df['seller_province'] = df.apply(
    lambda row: most_common_province_by_category.get(row['category'], row['seller_province']) if pd.notna(row['category']) and pd.isna(row['seller_province']) else row['seller_province'], axis=1
)

df.dropna(subset=['seller_province'],inplace=True)

# Fill the remaining of missing values in seller_region by newly added values in seller_province
df['seller_region'] = df.apply(fill_region_by_province, axis=1)

df.isnull().sum()

"""Based on the Chi-Square Test, we can conclude that product_name and category have a significant association with one another. Therefore, to determine and impute the null values in each column with high confidence, we can map each category to its corresponding 5 product_name values and vice versa to fill in as many missing values as possible.

Next, we continue to fill in missing values in the category column based on its association with nmv, as identified by the ANOVA Test. Although product_name and category are associated with other features such as price, gmv, shipping_fee_value, and discounted_value, we chose to use nmv because it is calculated as the final outcome of those elements, making it a more comprehensive indicator. After that, we use the newly imputed values in the category column to further map and fill in missing values in the product_name column as we did in the first step. Finally, we will drop any remaining null values if they still exist. The reason we chose to fill in the category column instead of product_name based on nmv is that category is the parent column of product_name. Therefore, it is more optimal and easier to impute a column with fewer unique values.
"""

df[['product_name','category']].groupby(['category','product_name']).count()

category_product_mapping = {
    'Electronics': {
        'product_name': ['Smartphone', 'Laptop', 'Tablet', 'Smartwatch', 'Headphones']
    },
    'Fashion': {
        'product_name': ['T-Shirt', 'Jeans', 'Sneakers', 'Jacket', 'Sunglasses']
    },
    'Sports': {
        'product_name': ['Basketball', 'Soccer Ball', 'Tennis Racket', 'Running Shoes', 'Yoga Mat']
    },
    'Toys': {
        'product_name': ['Action Figure', 'Doll', 'Lego Set', 'Puzzle', 'Board Game']
    },
    'Furniture': {
        'product_name': ['Sofa', 'Dining Table', 'Office Chair', 'Bed Frame', 'Bookshelf']
    },
    'Pet Supplies': {
        'product_name': ['Dog Food', 'Cat Litter', 'Bird Cage', 'Pet Bed', 'Fish Tank']
    },
    'Office Supplies': {
        'product_name': ['Printer', 'Desk', 'Notebook', 'Pen Set', 'Stapler']
    },
    'Tools & Hardware': {
        'product_name': ['Drill', 'Hammer', 'Screwdriver Set', 'Wrench', 'Measuring Tape']
    },
    'Outdoor Equipment': {
        'product_name': ['Tent', 'Sleeping Bag', 'Hiking Boots', 'Backpack', 'Water Bottle']
    },
    'Travel Accessories': {
        'product_name': ['Luggage', 'Passport Holder', 'Travel Pillow', 'Packing Cubes', 'Travel Adapter']
    }
}

# Create a reverse mapping of product to category
product_to_category = {}
for category, products in category_product_mapping.items():
    for product in products['product_name']:
        product_to_category[product] = category

# Function to fill null values in category using product_name
def fill_category_by_product(row):
    if pd.isna(row['category']) and pd.notna(row['product_name']):
        return product_to_category.get(row['product_name'], row['category'])
    return row['category']

# First fill null values in category using product_name
df['category'] = df.apply(fill_category_by_product, axis=1)

# Function to fill null values in product_name using category
def fill_product_by_category(row):
    if pd.isna(row['product_name']) and pd.notna(row['category']):
        return np.random.choice(category_product_mapping[row['category']]['product_name']) if row['category'] in category_product_mapping else row['product_name']
    return row['product_name']

# Then fill null values in product_name using category
df['product_name'] = df.apply(fill_product_by_category, axis=1)

df.isnull().sum()

df[df['category'].isna() & df['product_name'].isna()]

# Calculate the mean nmv for each category
mean_nmv_by_category = df.groupby('category')['nmv'].mean()

# Function to fill missing category values based on nmv
def fill_category_by_nmv(row):
    if pd.isna(row['category']) and pd.notna(row['nmv']):
        # Find the category whose mean nmv is closest to the row's nmv
        closest_category = (mean_nmv_by_category - row['nmv']).abs().idxmin()
        return closest_category
    return row['category']

df['category'] = df.apply(fill_category_by_nmv, axis=1)

# Fill the rest null values in product_name using newly added values in category
df['product_name'] = df.apply(fill_product_by_category, axis=1)

# Calculate the mean nmv for each order_size
mean_nmv_by_order_size = df.groupby('order_size')['nmv'].mean()

# Function to fill missing order_size values based on nmv
def fill_order_size_by_nmv(row):
    if pd.isna(row['order_size']) and pd.notna(row['nmv']):
        # Find the order_size whose mean nmv is closest to the row's nmv
        closest_order_size = (mean_nmv_by_order_size - row['nmv']).abs().idxmin()
        return closest_order_size
    return row['order_size']

df['order_size'] = df.apply(fill_order_size_by_nmv, axis=1)

# Calculate the mean nmv for each payment_method
mean_nmv_by_payment_method = df.groupby('payment_method')['nmv'].mean()

# Function to fill missing payment_method values based on nmv
def fill_payment_method_by_nmv(row):
    if pd.isna(row['payment_method']) and pd.notna(row['nmv']):
        # Find the payment_method whose mean nmv is closest to the row's nmv
        closest_payment_method = (mean_nmv_by_payment_method - row['nmv']).abs().idxmin()
        return closest_payment_method
    return row['payment_method']

df['payment_method'] = df.apply(fill_payment_method_by_nmv, axis=1)

df.isnull().sum()

df.info()

df.to_csv('/content/drive/MyDrive/Portfolio/1. Business Cases/1. Classification/Logistics/Project 1: On-Time Delivery/Data/cleaned_sales_orders.csv',index=False)

"""## Section 5: Feature Selection (Training Set)

Observations About other Numerical Variables

1.   **Input-Output:** All numerical columns are not associated with on_time_delivery column
2.   **Input-Input:** All the columns (price, qty_sold, shipping_fee_value, discounted_percent, discounted_value, gmv) are all associated to nmv.
3.   **Consideration:** It is possible to drop all price, qty_sold, shipping_fee_value, discounted_percent, discounted_value, gmv columns to avoid multicollinearity
4.   **Recommendation:** Drop all of them
"""

df.columns

df.drop(columns=['order_id','customer_id','seller_id','product_id','product_listed_on',
                 'price','qty_sold', 'gmv', 'shipping_fee_value',
                 'discounted_percent', 'discounted_value'],inplace=True)

df.head()

"""## Section 6: Dataset Splitting"""

from sklearn.model_selection import train_test_split

df.columns

x = df[['order_date', 'order_status', 'is_2h_delivery', 'is_cancelled',
       'seller_region', 'seller_province', 'product_name', 'category', 'nmv',
       'order_size', 'payment_method']]
y = df[['on_time_delivery']]

x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=40)

test_set = pd.concat([x_test,y_test],axis=1)

test_set.to_csv('/content/drive/MyDrive/Portfolio/1. Business Cases/1. Classification/Logistics/Project 1: On-Time Delivery/Data/test_sales_orders.csv',index=False)

train_set = pd.concat([x_train,y_train],axis=1)

train_set.head()

"""## Section 7: Data Preprocessing (Training Set)"""

from sklearn import preprocessing

cat_col

# order_status
train_set['order_status'].unique()

order_status_map = {'cancelled':0,'returned':1,'processing':2,'completed':3}
train_set['order_status_mapped'] = train_set['order_status'].map(order_status_map)
order_status_label_encoder = preprocessing.LabelEncoder()
order_status_label_encoder.fit(train_set['order_status_mapped'])
train_set['order_status_encoder'] = order_status_label_encoder.transform(train_set['order_status_mapped'])

train_set[['order_status','order_status_encoder']].head()

train_set.drop(columns=['order_status_mapped','order_status'],inplace=True)

# category
train_set['category'].unique()

# Order the categories by their frequency in ascending order
category_order = train_set['category'].value_counts(ascending=True).index

sns.countplot(data=train_set,x='category',order=category_order)
plt.xticks(rotation=60)
plt.show()

train_set['category'] = train_set['category'].replace(['Travel Accessories',
                                                       'Outdoor Equipment',
                                                       'Tools & Hardware',
                                                       'Office Supplies',
                                                       'Pet Supplies',
                                                       'Furniture'],'Other')

sns.countplot(data=train_set,x='category')
plt.xticks(rotation=60)
plt.show()

train_set[['category','on_time_delivery']].groupby('category',as_index=False).count()

category_map = {'Toys':0,'Sports':1,'Fashion':2,'Electronics':3,'Other':4}
train_set['category_mapped'] = train_set['category'].map(category_map)
category_label_encoder = preprocessing.LabelEncoder()
category_label_encoder.fit(train_set['category_mapped'])
train_set['category_encoder'] = category_label_encoder.transform(train_set['category_mapped'])

# product_name
train_set['product_name'].unique()

# product_name mappings for the "Other" category
other_product_names = [
    'Luggage', 'Passport Holder', 'Travel Pillow', 'Packing Cubes', 'Travel Adapter',
    'Tent', 'Sleeping Bag', 'Hiking Boots', 'Backpack', 'Water Bottle',
    'Drill', 'Hammer', 'Screwdriver Set', 'Wrench', 'Measuring Tape',
    'Printer', 'Desk', 'Notebook', 'Pen Set', 'Stapler',
    'Dog Food', 'Cat Litter', 'Bird Cage', 'Pet Bed', 'Fish Tank',
    'Sofa', 'Dining Table', 'Office Chair', 'Bed Frame', 'Bookshelf'
]

# Replace all product_name values that are in the "Other" category
train_set.loc[train_set['category'] == 'Other', 'product_name'] = train_set['product_name'].apply(
    lambda x: 'Other Products' if x in other_product_names else x
)

sns.countplot(data=train_set,x='product_name')
plt.xticks(rotation=90)
plt.show()

from sklearn.preprocessing import LabelEncoder

# Label encoding for product_name
product_name_encoder = LabelEncoder()
train_set['product_name_encoder'] = product_name_encoder.fit_transform(train_set['product_name'])

# Get unique values for product_name
unique_product_names = train_set['product_name'].unique()

# Display the unique values
print("Unique Product Names:", unique_product_names)

# Get unique values for product_name_encoded
unique_product_name_encoded = train_set['product_name_encoder'].unique()

# Display the unique values
print("Unique Encoded Product Names:", unique_product_name_encoded)

train_set.columns

train_set.drop(columns=['product_name','category_mapped','category'],inplace=True)

# order_size
train_set['order_size'].unique()

order_size_map = {'small':0,'medium':1,'big':2}
train_set['order_size_mapped'] = train_set['order_size'].map(order_size_map)
order_size_label_encoder = preprocessing.LabelEncoder()
order_size_label_encoder.fit(train_set['order_size_mapped'])
train_set['order_size_encoder'] = category_label_encoder.transform(train_set['order_size_mapped'])

train_set.drop(columns=['order_size_mapped','order_size'],inplace=True)

# payment_method
train_set['payment_method'].unique()

payment_method_map = {'Digital Wallets':0,'Cards':1,'COD':2,'Loyalty Points':3}
train_set['payment_method_mapped'] = train_set['payment_method'].map(payment_method_map)
payment_method_label_encoder = preprocessing.LabelEncoder()
payment_method_label_encoder.fit(train_set['payment_method_mapped'])
train_set['payment_method_encoder'] = payment_method_label_encoder.transform(train_set['payment_method_mapped'])

train_set.drop(columns=['payment_method_mapped','payment_method'],inplace=True)

train_set.columns

# seller_region
train_set['seller_region'].unique()

# Order the seller_region by their frequency in ascending order
seller_region_order = train_set['seller_region'].value_counts(ascending=True).index

sns.countplot(data=train_set,x='seller_region',order=seller_region_order)
plt.xticks(rotation=60)
plt.show()

train_set['seller_region'] = train_set['seller_region'].replace(['Seattle',
                                                       'Atlanta',
                                                       'Dallas',
                                                       'Boston',
                                                       'San Francisco',
                                                       'Miami'],'Other')

sns.countplot(data=train_set,x='seller_region')
plt.xticks(rotation=60)
plt.show()

# List of regions that should be grouped as "Other"
other_regions = ['Seattle', 'Atlanta', 'Dallas', 'Boston', 'San Francisco', 'Miami']

# List of provinces corresponding to the "Other" regions
other_provinces = [
    # Seattle
    'King County', 'Snohomish County', 'Pierce County', 'Kitsap County', 'Thurston County',
    # Atlanta
    'Fulton County', 'DeKalb County', 'Cobb County', 'Gwinnett County', 'Clayton County',
    # Dallas
    'Dallas County', 'Collin County', 'Denton County', 'Tarrant County', 'Rockwall County',
    # Boston
    'Suffolk County', 'Middlesex County', 'Norfolk County', 'Essex County', 'Plymouth County',
    # San Francisco
    'San Francisco County', 'San Mateo County', 'Alameda County', 'Marin County', 'Contra Costa County',
    # Miami
    'Miami-Dade County', 'Broward County', 'Palm Beach County', 'Monroe County', 'Collier County'
]

# Replace seller_region values that belong to "Other"
train_set['seller_region'] = train_set['seller_region'].apply(
    lambda x: 'Other' if x in other_regions else x
)

# Replace seller_province values that belong to the "Other" regions
train_set.loc[train_set['seller_region'] == 'Other', 'seller_province'] = train_set['seller_province'].apply(
    lambda x: 'Other Provinces' if x in other_provinces else x
)

print("Unique seller_province values:", train_set['seller_province'].unique())

train_set['seller_region'].unique()

# seller_region
seller_region_map = {'New York':0,'Los Angeles':1,'Chicago':2,'Houston':3,'Other':4}
train_set['seller_region_mapped'] = train_set['seller_region'].map(seller_region_map)
seller_region_label_encoder = preprocessing.LabelEncoder()
seller_region_label_encoder.fit(train_set['seller_region_mapped'])
train_set['seller_region_encoder'] = seller_region_label_encoder.transform(train_set['seller_region_mapped'])

# Label encoding for seller_province
seller_province_encoder = LabelEncoder()
train_set['seller_province_encoder'] = seller_province_encoder.fit_transform(train_set['seller_province'])

train_set.drop(columns=['seller_region','seller_province','seller_region_mapped'],inplace=True)

train_set.columns

num_col

print('Kurtosis: ',train_set['nmv'].skew())
print('Kurtosis: ',train_set['nmv'].kurt())

# Calculate Q1, Q3, IQR
Q1 = train_set['nmv'].quantile(0.25)
Q3 = train_set['nmv'].quantile(0.8)
IQR = Q3 - Q1

# Define the lower bound and upper bound
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove Outliers
train_set = train_set[(train_set['nmv'] >= lower_bound) & (train_set['nmv'] <= upper_bound)]

print('Kurtosis: ',train_set['nmv'].skew())
print('Kurtosis: ',train_set['nmv'].kurt())

sns.histplot(data=train_set,x='nmv')
plt.show()

train_set['nmv_binned'] = pd.qcut(df['nmv'], q=5)

train_set[['nmv_binned','on_time_delivery']].groupby('nmv_binned',as_index=False).count()

train_set.head()

train_set['nmv_binned_num'] = np.NaN

# Assign numerical labels based on the bins
train_set.loc[(train_set['nmv'] > 1.009) & (train_set['nmv'] <= 1508.298), 'nmv_binned_num'] = 0
train_set.loc[(train_set['nmv'] > 1508.298) & (train_set['nmv'] <= 11364.884), 'nmv_binned_num'] = 1
train_set.loc[(train_set['nmv'] > 11364.884) & (train_set['nmv'] <= 18867.78), 'nmv_binned_num'] = 2
train_set.loc[(train_set['nmv'] > 18867.78) & (train_set['nmv'] <= 44551.41), 'nmv_binned_num'] = 3
train_set.loc[train_set['nmv'] > 44551.41, 'nmv_binned_num'] = 4

train_set[['nmv_binned_num', 'on_time_delivery']].groupby('nmv_binned_num', as_index=False).count()

train_set.drop(columns=['nmv_binned','nmv','order_date'],inplace=True)

train_set.head()

train_set.columns

"""## Section 8: Dataset Splitting for Model Training (Training Set)"""

x = train_set[['is_2h_delivery', 'is_cancelled',
       'order_status_encoder', 'category_encoder', 'product_name_encoder',
       'order_size_encoder', 'payment_method_encoder', 'seller_region_encoder',
       'seller_province_encoder', 'nmv_binned_num']]
y = train_set[['on_time_delivery']]

x_train, x_test, y_train, y_test =  train_test_split(x,y,test_size=0.2,random_state=40)

"""## Section 9: Prediction (Training Set)"""

from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""### Logistic Regression"""

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()

logreg.fit(x_train,y_train)

y_pred = logreg.predict(x_test)

accuracy_logreg = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_logreg)

"""### KNN"""

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=5)

knn.fit(x_train,y_train)

y_pred = knn.predict(x_test)

accuracy_knn = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_knn)

"""### Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

decision_tree = DecisionTreeClassifier()

decision_tree.fit(x_train,y_train)

y_pred = decision_tree.predict(x_test)

accuracy_decision_tree = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_decision_tree)

"""### Random Forest"""

from sklearn.ensemble import RandomForestClassifier

random_forest = RandomForestClassifier()

random_forest.fit(x_train,y_train)

y_pred = random_forest.predict(x_test)

accuracy_random_forest = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_random_forest)

"""### Naive Bayes"""

from sklearn.naive_bayes import GaussianNB

naive_bayes = GaussianNB()

naive_bayes.fit(x_train,y_train)

y_pred = naive_bayes.predict(x_test)

accuracy_nb = accuracy_score(y_test,y_pred)
print('Training Set Accuracy: ',accuracy_nb)

pred_values = pd.DataFrame(
    {'Algorithms':['LogisticRegression','KNN','DecisionTree','RandomForest','Naive Bayes'],
    'Accuracy Score':[accuracy_logreg,accuracy_knn,accuracy_decision_tree,accuracy_random_forest,accuracy_nb]}
)

pred_values

"""**Comment:**

  * Since Logistic Regression, and Naive Bayes have the same accuracy score, we will pick the Logistic Regression for simplicity
"""

# K-fold cross-validation

kf = KFold(n_splits=5, shuffle=True, random_state=40)

# Cross-validation scores Logistic Regression
cv_scores_logreg = cross_val_score(logreg, x, y, cv=kf, scoring='accuracy')

print('Cross-validation scores: ', cv_scores_logreg)
print('Average cross-validation accuracy: ', cv_scores_logreg.mean())

# Confusion Matrix

print('Confusion Matrix:')
print(confusion_matrix(y_test, y_pred))

"""*   True Negatives (TN): 0
*   False Positives (FP): 1389
*   False Negatives (FN): 0
*   True Positives (TP): 12784
"""

# Classification report

print('Classification Report:')
print(classification_report(y_test, y_pred))

"""## Section 10: Data Preprocessing (Test Set)"""

test_set = pd.read_csv('/content/drive/MyDrive/Portfolio/1. Business Cases/1. Classification/Logistics/Project 1: On-Time Delivery/Data/test_sales_orders.csv')
test_set.head()

# order_status
order_status_map = {'cancelled':0,'returned':1,'processing':2,'completed':3}
test_set['order_status_mapped'] = test_set['order_status'].map(order_status_map)
order_status_label_encoder = preprocessing.LabelEncoder()
order_status_label_encoder.fit(test_set['order_status_mapped'])
test_set['order_status_encoder'] = order_status_label_encoder.transform(test_set['order_status_mapped'])

test_set.drop(columns=['order_status_mapped','order_status'],inplace=True)

# category
test_set['category'] = test_set['category'].replace(['Travel Accessories',
                                                       'Outdoor Equipment',
                                                       'Tools & Hardware',
                                                       'Office Supplies',
                                                       'Pet Supplies',
                                                       'Furniture'],'Other')

category_map = {'Toys':0,'Sports':1,'Fashion':2,'Electronics':3,'Other':4}
test_set['category_mapped'] = test_set['category'].map(category_map)
category_label_encoder = preprocessing.LabelEncoder()
category_label_encoder.fit(test_set['category_mapped'])
test_set['category_encoder'] = category_label_encoder.transform(test_set['category_mapped'])

# product_name mappings for the "Other" category
other_product_names = [
    'Luggage', 'Passport Holder', 'Travel Pillow', 'Packing Cubes', 'Travel Adapter',
    'Tent', 'Sleeping Bag', 'Hiking Boots', 'Backpack', 'Water Bottle',
    'Drill', 'Hammer', 'Screwdriver Set', 'Wrench', 'Measuring Tape',
    'Printer', 'Desk', 'Notebook', 'Pen Set', 'Stapler',
    'Dog Food', 'Cat Litter', 'Bird Cage', 'Pet Bed', 'Fish Tank',
    'Sofa', 'Dining Table', 'Office Chair', 'Bed Frame', 'Bookshelf'
]

# Replace all product_name values that are in the "Other" category
test_set.loc[test_set['category'] == 'Other', 'product_name'] = test_set['product_name'].apply(
    lambda x: 'Other Products' if x in other_product_names else x
)

from sklearn.preprocessing import LabelEncoder

# Label encoding for product_name
product_name_encoder = LabelEncoder()
test_set['product_name_encoder'] = product_name_encoder.fit_transform(test_set['product_name'])

# Get unique values for product_name
unique_product_names = test_set['product_name'].unique()

# Display the unique values
print("Unique Product Names:", unique_product_names)

# Get unique values for product_name_encoded
unique_product_name_encoded = test_set['product_name_encoder'].unique()

# Display the unique values
print("Unique Encoded Product Names:", unique_product_name_encoded)

test_set.drop(columns=['product_name','category_mapped','category'],inplace=True)

# order_size
order_size_map = {'small':0,'medium':1,'big':2}
test_set['order_size_mapped'] = test_set['order_size'].map(order_size_map)
order_size_label_encoder = preprocessing.LabelEncoder()
order_size_label_encoder.fit(test_set['order_size_mapped'])
test_set['order_size_encoder'] = category_label_encoder.transform(test_set['order_size_mapped'])

test_set.drop(columns=['order_size_mapped','order_size'],inplace=True)

# payment_method

payment_method_map = {'Digital Wallets':0,'Cards':1,'COD':2,'Loyalty Points':3}
test_set['payment_method_mapped'] = test_set['payment_method'].map(payment_method_map)
payment_method_label_encoder = preprocessing.LabelEncoder()
payment_method_label_encoder.fit(test_set['payment_method_mapped'])
test_set['payment_method_encoder'] = payment_method_label_encoder.transform(test_set['payment_method_mapped'])

test_set.drop(columns=['payment_method_mapped','payment_method'],inplace=True)

# seller_region

test_set['seller_region'] = test_set['seller_region'].replace(['Seattle',
                                                       'Atlanta',
                                                       'Dallas',
                                                       'Boston',
                                                       'San Francisco',
                                                       'Miami'],'Other')

# List of regions that should be grouped as "Other"
other_regions = ['Seattle', 'Atlanta', 'Dallas', 'Boston', 'San Francisco', 'Miami']

# List of provinces corresponding to the "Other" regions
other_provinces = [
    # Seattle
    'King County', 'Snohomish County', 'Pierce County', 'Kitsap County', 'Thurston County',
    # Atlanta
    'Fulton County', 'DeKalb County', 'Cobb County', 'Gwinnett County', 'Clayton County',
    # Dallas
    'Dallas County', 'Collin County', 'Denton County', 'Tarrant County', 'Rockwall County',
    # Boston
    'Suffolk County', 'Middlesex County', 'Norfolk County', 'Essex County', 'Plymouth County',
    # San Francisco
    'San Francisco County', 'San Mateo County', 'Alameda County', 'Marin County', 'Contra Costa County',
    # Miami
    'Miami-Dade County', 'Broward County', 'Palm Beach County', 'Monroe County', 'Collier County'
]

# Replace seller_region values that belong to "Other"
test_set['seller_region'] = test_set['seller_region'].apply(
    lambda x: 'Other' if x in other_regions else x
)

# Replace seller_province values that belong to the "Other" regions
test_set.loc[test_set['seller_region'] == 'Other', 'seller_province'] = test_set['seller_province'].apply(
    lambda x: 'Other Provinces' if x in other_provinces else x
)

# seller_region
seller_region_map = {'New York':0,'Los Angeles':1,'Chicago':2,'Houston':3,'Other':4}
test_set['seller_region_mapped'] = test_set['seller_region'].map(seller_region_map)
seller_region_label_encoder = preprocessing.LabelEncoder()
seller_region_label_encoder.fit(test_set['seller_region_mapped'])
test_set['seller_region_encoder'] = seller_region_label_encoder.transform(test_set['seller_region_mapped'])

# Label encoding for seller_province
seller_province_encoder = LabelEncoder()
test_set['seller_province_encoder'] = seller_province_encoder.fit_transform(test_set['seller_province'])

test_set.drop(columns=['seller_region','seller_province','seller_region_mapped'],inplace=True)

# Calculate Q1, Q3, IQR
Q1 = test_set['nmv'].quantile(0.25)
Q3 = test_set['nmv'].quantile(0.8)
IQR = Q3 - Q1

# Define the lower bound and upper bound
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove Outliers
test_set = test_set[(test_set['nmv'] >= lower_bound) & (test_set['nmv'] <= upper_bound)]

test_set['nmv_binned'] = pd.qcut(df['nmv'], q=5)

test_set[['nmv_binned','on_time_delivery']].groupby('nmv_binned',as_index=False).count()

test_set['nmv_binned_num'] = np.NaN

# Assign numerical labels based on the bins
test_set.loc[(test_set['nmv'] > 1.009) & (test_set['nmv'] <= 1508.298), 'nmv_binned_num'] = 0
test_set.loc[(test_set['nmv'] > 1508.298) & (test_set['nmv'] <= 11364.884), 'nmv_binned_num'] = 1
test_set.loc[(test_set['nmv'] > 11364.884) & (test_set['nmv'] <= 18867.78), 'nmv_binned_num'] = 2
test_set.loc[(test_set['nmv'] > 18867.78) & (test_set['nmv'] <= 44551.41), 'nmv_binned_num'] = 3
test_set.loc[test_set['nmv'] > 44551.41, 'nmv_binned_num'] = 4

test_set.drop(columns=['nmv_binned','nmv','order_date'],inplace=True)

test_set.head()

test_set.columns

"""## Section 11: Prediction (Test Set)"""

# Make predictions on the test set
y_pred_test = logreg.predict(test_set[['is_2h_delivery', 'is_cancelled',
       'order_status_encoder', 'category_encoder', 'product_name_encoder',
       'order_size_encoder', 'payment_method_encoder', 'seller_region_encoder',
       'seller_province_encoder', 'nmv_binned_num']])

# Accuracy score

test_accuracy = accuracy_score(test_set['on_time_delivery'], y_pred_test)
print('Test Set Accuracy: ', test_accuracy)

# Confusion matrix

test_confusion_matrix = confusion_matrix(test_set['on_time_delivery'], y_pred_test)
print('Confusion Matrix:')
print(test_confusion_matrix)

# Classification report

test_classification_report = classification_report(test_set['on_time_delivery'], y_pred_test)
print('Classification Report:')
print(test_classification_report)